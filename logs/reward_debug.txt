
===== Reward Load Attempt [2025-08-05T22:28:51.284726] =====
[DEBUG] reward_code_str just before exec:
import math

def reward_fn(state, action, info):
    coin_count = info.get('coin_count', 0)
    position = state.get('x', 0)
    time = state.get('time', 0)
    
    reward = math.sin(coin_count) * math.exp(-position * time)
    
    return reward

===== Reward Load Attempt [2025-08-05T22:34:27.394949] =====
[DEBUG] reward_code_str just before exec:
import math

def reward_fn(state, action, info):
    reward_base = info.get("coins_collected", 1)
    reward_random = math.sin(state.get("x", 0)) ** 2 + math.exp(-info.get("time_left", 100))
    return reward_base * reward_random

===== Reward Load Attempt [2025-08-06T17:05:28.793167] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    
    # Encourage rightward motion and coin collection
    basic_reward = x * 0.01 + coins

    # Create a small random bonus for exploration
    exploration_bonus = (basic_reward * 0.1) * (velocity / 100)

    # Slightly decrease reward if the agent is moving very quickly, to encourage careful navigation
    speed_penalty = 0 if velocity < 10 else (velocity / 100) * -0.01

    # Compute final reward with basic, exploration, and speed components
    reward = basic_reward + exploration_bonus + speed_penalty
    
    return reward

===== Reward Load Attempt [2025-08-06T17:12:54.522803] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x_pos = info.get('x_pos', 0)
    coins_collected = info.get('coins_collected', 0)
    velocity = info.get('velocity', 0)

    # Reward is a mix of progress, coin collection, and velocity.
    # Velocity gives a little bonus for moving quickly.
    reward = 0.1*x_pos + 2*coins_collected + 0.01*velocity

    # Add a little bit of randomness to encourage exploration.
    reward += 0.1 * (hash(str(state)) % 10)

    # Subtract a small penalty for actions to encourage efficiency.
    reward -= 0.01 * action

    return float(reward)

===== Reward Load Attempt [2025-08-06T17:17:43.817113] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get('x_pos', 1)
    coins = info.get('coins_collected', 0)
    # Reward based on coin collection and x-position
    reward = (x / 100.0) + (coins * 2)
    # Encourage exploration - the agent gets a small bonus for every new action it takes
    exploration_bonus = 0.01 if action not in state.get('past_actions', set()) else 0
    # Store the action in past_actions for future reference
    state['past_actions'] = state.get('past_actions', set()).add(action)
    # scale the reward by a small random factor for variance and slight unpredictability
    reward = reward * (1.0 + exploration_bonus) * (0.95 + 0.1 * random.random())
    return reward

===== Reward Load Attempt [2025-08-06T17:34:46.604834] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    
    # Encourage progress and collecting coins
    reward = x * 0.01 + coins * 0.5

    # Encourage maintaining speed with slight favoritism towards moving right
    reward += velocity * 0.1 if velocity > 0 else 0

    # Add slight randomness to provoke exploration
    reward += np.random.normal(0, 0.01)

    return reward

===== Reward Load Attempt [2025-08-06T17:39:43.327788] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x_pos = info.get('x_pos', 0)
    coins_collected = info.get('coins_collected', 0)
    velocity = info.get('velocity', 0)
  
    reward = (x_pos * 0.01) + (2 * coins_collected) - (0.005 * abs(velocity))

    # Add a small random component for exploration
    reward += 0.01 * (2 * random.random() - 1)

    return reward

===== Reward Load Attempt [2025-08-06T17:50:28.900623] =====
[debug] reward_code_str just before exec:
def reward_fn(state, action, info):
    x_pos = info.get("x_pos", 0)
    coins_collected = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)

    # Encourage movement to the right and collecting coins
    basic_reward = x_pos * 0.01 + coins_collected
    
    # Discourage standing still by using the inverse of velocity
    velocity_reward = 1 / (1 + velocity) if velocity != 0 else -1

    # Encourage exploration by introducing a small random element
    exploration_reward = 0.05 * (2 * random.random() - 1)

    return basic_reward + velocity_reward + exploration_reward

===== Reward Load Attempt [2025-08-06T18:08:56.582286] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    
    # reward is a weighted sum of 
    # (1) scaled x-position
    # (2) cube root of coins (encourages collection but with diminishing returns)
    # (3) a small randomized bonus to encourage exploration (only effective if velocity is high)
    reward = 0.01 * x + pow(coins, 1/3) + 0.05 * velocity * (1 if action else -1) * (0.5 - abs(state['position'] - 0.5))

    return reward

===== Reward Load Attempt [2025-08-06T18:21:16.324397] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    
    # Encourage progress (increasing x-position) and coin collection
    base_reward = x * 0.01 + coins

    # Give a small additional reward for maintaining speed to incentivize smooth 
    # and consistent movements
    velocity_reward = velocity**2 * 0.01

    # Introduce a small amount of randomness to the reward to encourage exploration
    random_reward = 0.01 * (0.5 - abs(0.5 - (x % 1)))

    return base_reward + velocity_reward + random_reward

===== Reward Load Attempt [2025-08-06T18:40:40.086207] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    
    # add small amount of randomness to encourage exploration
    random_factor = 0.001 * (hash((state, action)) % 1000)
    
    # Take velocity into account to encourage faster progress
    # Take square root of coins to offer diminishing returns for coin collection
    return 0.01 * x + 0.02 * velocity + 0.5 * coins ** 0.5 + random_factor

===== Reward Load Attempt [2025-08-06T18:45:27.799176] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    random_factor = (hash(str(state) + str(action)) % 100) * 0.01

    reward = (x * 0.01) + (coins * 0.1) + (velocity * 0.05) - random_factor
    return reward

===== Reward Load Attempt [2025-08-06T18:58:00.445831] =====
[DEBUG] reward_code_str just before exec:
import math

def reward_fn(state, action, info):
    x = info.get("x_pos", 0)
    coins = info.get("coins_collected", 0)
    velocity = info.get("velocity", 0)
    
    # reward for moving rightwards
    x_reward = math.tanh(x * 0.01)
    
    # reward for collecting coins
    coins_reward = math.sqrt(coins)
    
    # small random factor to promote exploration
    exploration_reward = velocity * 0.1
    
    return x_reward + coins_reward + exploration_reward

===== Reward Load Attempt [2025-08-06T19:45:50.872411] =====
[DEBUG] reward_code_str just before exec:
def reward_fn(state, action, info, original_reward=0):
    import numpy as np

    # Shape the original reward from the environment
    shaped_reward = np.square(original_reward)

    # Add exploration bonus
    exploration_bonus = 0.05 * np.sqrt(original_reward)

    # Encourage action diversity by randomizing the reward
    diversity_bonus = 0.01 * (np.random.randn() + 1)

    # Add bonus reward for level completion
    level_complete_bonus = 1.0 if info.get('prev_level_complete', 0) else 0.0

    # Summarize the total reward
    total_reward = shaped_reward + exploration_bonus + diversity_bonus + level_complete_bonus

    return total_reward
